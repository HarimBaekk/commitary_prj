"""
RAG ê³ ë„í™” ì „/í›„ ë¹„êµ í…ŒìŠ¤íŠ¸
"""
import json
import sys
import os
import time
import re
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

load_dotenv()
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

from commitary_backend.app import create_app

# í…ŒìŠ¤íŠ¸ ì„¤ì •
TEST_REPO_ID = 1061647946
TEST_BRANCH = "main"
TEST_COMMITARY_ID = 1
TEST_DATE_STR = "2025-09-30"


def delete_existing_insight(app, test_date_str):
    """ê¸°ì¡´ ì¸ì‚¬ì´íŠ¸ ì‚­ì œ"""
    print(f"\nğŸ—‘ï¸ ê¸°ì¡´ ì¸ì‚¬ì´íŠ¸ ì‚­ì œ ì¤‘ (ë‚ ì§œ: {test_date_str})...")
    
    with app.app_context():
        from commitary_backend.commitaryUtils.dbConnectionDecorator import get_db_conn
        
        conn = get_db_conn()
        try:
            with conn.cursor() as cur:
                cur.execute("""
                    DELETE FROM insight_item 
                    WHERE daily_insight_id IN (
                        SELECT daily_insight_id 
                        FROM daily_insight 
                        WHERE date = %s AND repo_id = %s
                    )
                """, (test_date_str, TEST_REPO_ID))
                
                deleted_items = cur.rowcount
                
                cur.execute("""
                    DELETE FROM daily_insight 
                    WHERE date = %s AND repo_id = %s
                """, (test_date_str, TEST_REPO_ID))
                
                deleted_insights = cur.rowcount
                
                conn.commit()
                print(f"  âœ… ì‚­ì œ ì™„ë£Œ: {deleted_insights}ê°œ ì¸ì‚¬ì´íŠ¸, {deleted_items}ê°œ ì•„ì´í…œ")
        except Exception as e:
            conn.rollback()
            print(f"  âŒ ì‚­ì œ ì‹¤íŒ¨: {e}")


def create_insight_with_version(app, version="OLD"):
    """íŠ¹ì • ë²„ì „ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„± + ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    print(f"\nğŸ“Š {version} ë²„ì „ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
    
    metrics = {
        "total_time": 0,
        "embedding_tokens": 0,
        "embedding_calls": 0,
        "llm_tokens": 0,
        "llm_prompt_tokens": 0,
        "llm_completion_tokens": 0
    }
    
    with app.app_context():
        if version == "OLD":
            from commitary_backend.services.insightService.OLD.InsightServiceObject_OLD import InsightService
        else:
            from commitary_backend.services.insightService.InsightServiceObject import InsightService
        
        insight_service = InsightService()
        
        import logging
        from io import StringIO
        
        log_capture = StringIO()
        handler = logging.StreamHandler(log_capture)
        handler.setLevel(logging.DEBUG)
        app.logger.addHandler(handler)
        
        try:
            test_datetime = datetime.strptime(TEST_DATE_STR, "%Y-%m-%d")
            test_datetime = test_datetime.replace(hour=12, minute=0, second=0, tzinfo=timezone.utc)
            
            print(f"  ë ˆí¬: Seongbong-Ha/dotodo_backend (ID: {TEST_REPO_ID})")
            print(f"  ë‚ ì§œ: {TEST_DATE_STR}")
            print(f"  ë¸Œëœì¹˜: {TEST_BRANCH}")
            
            start_time = time.time()
            
            status = insight_service.createDailyInsight(
                commitary_id=TEST_COMMITARY_ID,
                repo_id=TEST_REPO_ID,
                start_datetime=test_datetime,
                branch=TEST_BRANCH,
                user_token=GITHUB_TOKEN
            )
            
            metrics["total_time"] = time.time() - start_time
            
            log_contents = log_capture.getvalue()
            
            # ì„ë² ë”© í† í° ì¶”ì¶œ
            embedding_pattern = r"Token count \(estimated\): (\d+)"
            embedding_matches = re.findall(embedding_pattern, log_contents)
            if embedding_matches:
                metrics["embedding_tokens"] = sum(int(x) for x in embedding_matches)
                metrics["embedding_calls"] = len(embedding_matches)
            
            # LLM í† í° ì¶”ì¶œ
            llm_pattern = r"Total Tokens: (\d+).*?Prompt Tokens: (\d+).*?Completion Tokens: (\d+)"
            llm_matches = re.findall(llm_pattern, log_contents, re.DOTALL)
            if llm_matches:
                total, prompt, completion = llm_matches[-1]
                metrics["llm_tokens"] = int(total)
                metrics["llm_prompt_tokens"] = int(prompt)
                metrics["llm_completion_tokens"] = int(completion)
            
            print(f"  ìƒì„± ìƒíƒœ: {status}")
            
            status_messages = {
                0: "âœ… ì¸ì‚¬ì´íŠ¸ ìƒì„± ì„±ê³µ",
                1: "â„¹ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¸ì‚¬ì´íŠ¸",
                -1: "âš ï¸ í™œë™ ì—†ìŒ",
                2: "âŒ ìƒì„± ì‹¤íŒ¨"
            }
            print(f"  {status_messages.get(status, 'â“ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ')}")
            
            print(f"\nâ±ï¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
            print(f"  ì „ì²´ ì‹œê°„: {metrics['total_time']:.2f}ì´ˆ")
            print(f"  ì„ë² ë”© í† í°: {metrics['embedding_tokens']:,}ê°œ ({metrics['embedding_calls']}íšŒ)")
            print(f"  LLM í† í°: {metrics['llm_tokens']:,}ê°œ (í”„ë¡¬í”„íŠ¸: {metrics['llm_prompt_tokens']:,}, ì‘ë‹µ: {metrics['llm_completion_tokens']:,})")
            
            if status not in [0, 1]:
                return {
                    "status": "error",
                    "message": f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨ (status: {status})",
                    "metrics": metrics
                }
            
            start_dt = test_datetime - timedelta(days=1)
            end_dt = test_datetime + timedelta(days=1)
            
            insights_dto = insight_service.getInsights(
                commitary_id=TEST_COMMITARY_ID,
                repo_id=TEST_REPO_ID,
                start_datetime=start_dt,
                end_datetime=end_dt
            )
            
            for insight in insights_dto.insights:
                insight_date_str = insight.date_of_insight.strftime("%Y-%m-%d")
                if insight_date_str == TEST_DATE_STR and insight.activity:
                    for item in insight.items:
                        if item.branch_name == TEST_BRANCH:
                            return {
                                "status": "success",
                                "date": TEST_DATE_STR,
                                "branch": TEST_BRANCH,
                                "insight": item.insight,
                                "length": len(item.insight),
                                "metrics": metrics
                            }
            
            return {
                "status": "not_found",
                "message": "ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "metrics": metrics
            }
        
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return {
                "status": "error",
                "message": str(e),
                "metrics": metrics
            }
        
        finally:
            app.logger.removeHandler(handler)


def analyze_insights(old_result, new_result):
    """ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ë° ë¹„êµ"""
    print("\n" + "="*80)
    print("ğŸ“Š ì¸ì‚¬ì´íŠ¸ ë¹„êµ ë¶„ì„")
    print("="*80)
    
    if old_result["status"] != "success":
        print(f"\nâš ï¸ ê³ ë„í™” ì „ ì¸ì‚¬ì´íŠ¸ ì‹¤íŒ¨: {old_result.get('message', 'Unknown')}")
    
    if new_result["status"] != "success":
        print(f"\nâš ï¸ ê³ ë„í™” í›„ ì¸ì‚¬ì´íŠ¸ ì‹¤íŒ¨: {new_result.get('message', 'Unknown')}")
    
    if old_result["status"] != "success" or new_result["status"] != "success":
        return
    
    print(f"\nğŸ“ ì¸ì‚¬ì´íŠ¸ ê¸¸ì´ ë¹„êµ")
    print(f"  ê³ ë„í™” ì „: {old_result['length']:,} ì")
    print(f"  ê³ ë„í™” í›„: {new_result['length']:,} ì")
    diff = new_result['length'] - old_result['length']
    percent = (diff / old_result['length'] * 100) if old_result['length'] > 0 else 0
    print(f"  ì°¨ì´: {diff:+,} ì ({percent:+.1f}%)")
    
    print(f"\nğŸ“‹ êµ¬ì¡° ë¶„ì„")
    
    old_has_summary = 'ë³€ê²½ì‚¬í•­ ìš”ì•½' in old_result['insight']
    new_has_summary = 'ë³€ê²½ì‚¬í•­ ìš”ì•½' in new_result['insight']
    
    old_has_details = 'ì£¼ìš” ë³€ê²½' in old_result['insight']
    new_has_details = 'ì£¼ìš” ë³€ê²½' in new_result['insight']
    
    old_has_analysis = 'ê¸°ìˆ ì  ë¶„ì„' in old_result['insight']
    new_has_analysis = 'ê¸°ìˆ ì  ë¶„ì„' in new_result['insight']
    
    print(f"  ë³€ê²½ì‚¬í•­ ìš”ì•½: ê³ ë„í™” ì „ {'âœ…' if old_has_summary else 'âŒ'} | ê³ ë„í™” í›„ {'âœ…' if new_has_summary else 'âŒ'}")
    print(f"  ì£¼ìš” ë³€ê²½ ë‚´ì—­: ê³ ë„í™” ì „ {'âœ…' if old_has_details else 'âŒ'} | ê³ ë„í™” í›„ {'âœ…' if new_has_details else 'âŒ'}")
    print(f"  ê¸°ìˆ ì  ë¶„ì„: ê³ ë„í™” ì „ {'âœ…' if old_has_analysis else 'âŒ'} | ê³ ë„í™” í›„ {'âœ…' if new_has_analysis else 'âŒ'}")
    
    print(f"\n" + "="*80)
    print("ğŸ“„ ê³ ë„í™” ì „ ì¸ì‚¬ì´íŠ¸")
    print("="*80)
    print(old_result['insight'])
    
    print(f"\n" + "="*80)
    print("ğŸ“„ ê³ ë„í™” í›„ ì¸ì‚¬ì´íŠ¸")
    print("="*80)
    print(new_result['insight'])
    
    comparison = {
        "test_info": {
            "repository": "Seongbong-Ha/dotodo_backend",
            "repo_id": TEST_REPO_ID,
            "test_date": TEST_DATE_STR,
            "branch": TEST_BRANCH,
            "test_time": datetime.now().isoformat()
        },
        "before_optimization": {
            "method": "ê³ ë„í™” ì „",
            "config": {
                "chunking": "RecursiveCharacterTextSplitter (ì–¸ì–´ êµ¬ë¶„ ì—†ìŒ)",
                "chunk_size": 1000,
                "chunk_overlap": 150,
                "retrieval": "ë‹¨ìˆœ ìœ ì‚¬ë„ ê²€ìƒ‰",
                "retrieval_count": 3
            },
            "result": {
                "length": old_result['length'],
                "has_summary": old_has_summary,
                "has_details": old_has_details,
                "has_analysis": old_has_analysis,
                "insight": old_result['insight']
            },
            "performance": {
                "latency_seconds": round(old_result.get('metrics', {}).get('total_time', 0), 2),
                "embedding_tokens": old_result.get('metrics', {}).get('embedding_tokens', 0),
                "embedding_calls": old_result.get('metrics', {}).get('embedding_calls', 0),
                "llm_tokens": {
                    "total": old_result.get('metrics', {}).get('llm_tokens', 0),
                    "prompt": old_result.get('metrics', {}).get('llm_prompt_tokens', 0),
                    "completion": old_result.get('metrics', {}).get('llm_completion_tokens', 0)
                }
            }
        },
        "after_optimization": {
            "method": "ê³ ë„í™” í›„",
            "config": {
                "chunking": "ì–¸ì–´ë³„ RecursiveCharacterTextSplitter",
                "chunk_size": 1500,
                "chunk_overlap": 200,
                "retrieval": "íŒŒì¼ ê¸°ë°˜ í•„í„°ë§ (2-stage)",
                "retrieval_count": "3 (changed) + 2 (other) = 5"
            },
            "result": {
                "length": new_result['length'],
                "has_summary": new_has_summary,
                "has_details": new_has_details,
                "has_analysis": new_has_analysis,
                "insight": new_result['insight']
            },
            "performance": {
                "latency_seconds": round(new_result.get('metrics', {}).get('total_time', 0), 2),
                "embedding_tokens": new_result.get('metrics', {}).get('embedding_tokens', 0),
                "embedding_calls": new_result.get('metrics', {}).get('embedding_calls', 0),
                "llm_tokens": {
                    "total": new_result.get('metrics', {}).get('llm_tokens', 0),
                    "prompt": new_result.get('metrics', {}).get('llm_prompt_tokens', 0),
                    "completion": new_result.get('metrics', {}).get('llm_completion_tokens', 0)
                }
            }
        },
        "comparison": {
            "length_diff": diff,
            "length_diff_percent": round(percent, 2)
        }
    }
    
    if "metrics" in old_result and "metrics" in new_result:
        old_m = old_result["metrics"]
        new_m = new_result["metrics"]
        
        time_diff = new_m['total_time'] - old_m['total_time']
        time_pct = (time_diff / old_m['total_time'] * 100) if old_m['total_time'] > 0 else 0
        
        emb_diff = new_m['embedding_tokens'] - old_m['embedding_tokens']
        emb_pct = (emb_diff / old_m['embedding_tokens'] * 100) if old_m['embedding_tokens'] > 0 else 0
        
        llm_diff = new_m['llm_tokens'] - old_m['llm_tokens']
        llm_pct = (llm_diff / old_m['llm_tokens'] * 100) if old_m['llm_tokens'] > 0 else 0
        
        old_cost = (
            (old_m['embedding_tokens'] / 1000) * 0.0001 +
            (old_m['llm_prompt_tokens'] / 1000) * 0.03 +
            (old_m['llm_completion_tokens'] / 1000) * 0.06
        )
        
        new_cost = (
            (new_m['embedding_tokens'] / 1000) * 0.0001 +
            (new_m['llm_prompt_tokens'] / 1000) * 0.03 +
            (new_m['llm_completion_tokens'] / 1000) * 0.06
        )
        
        cost_diff = new_cost - old_cost
        cost_pct = (cost_diff / old_cost * 100) if old_cost > 0 else 0
        
        comparison["comparison"]["performance"] = {
            "latency_diff_seconds": round(time_diff, 2),
            "latency_diff_percent": round(time_pct, 2),
            "embedding_tokens_diff": emb_diff,
            "embedding_tokens_diff_percent": round(emb_pct, 2),
            "llm_tokens_diff": llm_diff,
            "llm_tokens_diff_percent": round(llm_pct, 2),
            "cost_diff_usd": round(cost_diff, 4),
            "cost_diff_percent": round(cost_pct, 2),
            "cost_before_usd": round(old_cost, 4),
            "cost_after_usd": round(new_cost, 4)
        }
    
    output_file = "insight_comparison_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ìƒì„¸ ë¹„êµ ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def delete_vector_snapshots(test_date_str, repo_id):
    """ë²¡í„° ìŠ¤ëƒ…ìƒ· ì‚­ì œ"""
    print(f"\nğŸ—‘ï¸ ë²¡í„° ìŠ¤ëƒ…ìƒ· ì‚­ì œ ì¤‘...")
    
    import psycopg2
    DATABASE_URL = os.getenv("DATABASE_URL")
    
    conn = psycopg2.connect(DATABASE_URL)
    try:
        # í•´ë‹¹ ì£¼ì˜ ì›”ìš”ì¼ ê³„ì‚°
        test_date = datetime.strptime(test_date_str, "%Y-%m-%d").date()
        monday_date = test_date - timedelta(days=test_date.weekday())
        snapshot_week_id = monday_date.isoformat()
        
        with conn.cursor() as cur:
            # OLD ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
            cur.execute("""
                DELETE FROM langchain_pg_embedding
                WHERE cmetadata->>'repo_id' = %s
                AND cmetadata->>'snapshot_week_id' = %s
                AND cmetadata->>'collection_name' = 'codebase_snapshots_OLD'
            """, (str(repo_id), snapshot_week_id))
            
            deleted_old = cur.rowcount
            
            # NEW ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
            cur.execute("""
                DELETE FROM langchain_pg_embedding
                WHERE cmetadata->>'repo_id' = %s
                AND cmetadata->>'snapshot_week_id' = %s
                AND cmetadata->>'collection_name' = 'codebase_snapshots_NEW'
            """, (str(repo_id), snapshot_week_id))
            
            deleted_new = cur.rowcount
            
            conn.commit()
            print(f"  âœ… ì‚­ì œ ì™„ë£Œ: OLD {deleted_old:,}ê°œ, NEW {deleted_new:,}ê°œ")
    except Exception as e:
        conn.rollback()
        print(f"  âŒ ì‚­ì œ ì‹¤íŒ¨: {e}")
    finally:
        conn.close()


def main():
    print("\n" + "="*80)
    print("RAG ì‹œìŠ¤í…œ ê³ ë„í™” ì „/í›„ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"\ní…ŒìŠ¤íŠ¸ ëŒ€ìƒ:")
    print(f"  ë ˆí¬ì§€í† ë¦¬: Seongbong-Ha/dotodo_backend")
    print(f"  ë‚ ì§œ: {TEST_DATE_STR}")
    print(f"  ë¸Œëœì¹˜: {TEST_BRANCH}")
    
    app = create_app()

    delete_vector_snapshots(TEST_DATE_STR, TEST_REPO_ID)
    
    delete_existing_insight(app, TEST_DATE_STR)
    
    old_result = create_insight_with_version(app, "OLD")
    
    print("\nğŸ—‘ï¸ OLD ì¸ì‚¬ì´íŠ¸ ì‚­ì œí•˜ê³  NEW ì¤€ë¹„...")
    delete_existing_insight(app, TEST_DATE_STR)
    
    new_result = create_insight_with_version(app, "NEW")
    
    analyze_insights(old_result, new_result)
    
    print("\nâœ… ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    if not GITHUB_TOKEN or GITHUB_TOKEN == "YOUR_PERSONAL_ACCESS_TOKEN":
        print("ERROR: GITHUB_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        main()